includeConfig "./tool_event.config"

// General configuration used in all profiles
manifest {
  description = 'APAeval Benchmark Workflow' 
  nextflowVersion = '>=19.10.0'
  version = '1.0.3'
}

// Profiles configure nextflow depending on the environment (local, integration, live, etc.)

profiles {

   docker {
      process {
          withName: validation{
            container = "docker.io/apaeval/rel_q_validation:1.0"
          }
      }
      process {
          withName: compute_metrics{
            container = "docker.io/apaeval/rel_q_metrics:1.0"
          }
      }
      process {
          withName: benchmark_consolidation{
            container = "docker.io/apaeval/rel_q_consolidation:1.0"
          }
      }
      
      docker.enabled = true
      // set time zone for running docker containers
      docker.runOptions = '--user \$(id -u):\$(id -g) -e TZ="\$([ -z \\"\$TZ\\"] && cat /etc/timezone || echo \\"\$TZ\\")"'
  }

  slurm {
      singularity.enabled = true
      singularity.autoMounts = true
      singularity.cacheDir = "${HOME}/.singularity/cache/library"
      singularity.libraryDir = "${HOME}/.singularity/cache/library"

      process {
              executor = 'slurm'
              memory = '4 GB'
              cpus = '1'

              withName: validation{
                        container = "docker.io/cjh4zavolab/rel_q_validation:1.0"
                        time = '30m'
              }
              withName: compute_metrics{
                        container = "docker.io/cjh4zavolab/rel_q_metrics:1.0"
                        time = '2h'
              }
              withName: benchmark_consolidation{
                        container = "docker.io/cjh4zavolab/rel_q_consolidation:1.0"
                        time = '30m'
              }
      }
  }
}

// default parameter values

params  {

  // Specify input files and challenge ids in separate tool_event.config; make sure you include that config here (at the top of the file)
  
  // directory where the 'gold standards' are found
  goldstandard_dir = "${params.indir}/ref"

  // directory where existing aggregations (e.g. downloaded from OEB) are stored
  aggregation_dir = "${params.indir}/aggr_data"

  //name or OEB permanent ID for the benchmarking community
  community_id = "APAeval"

  // Boolean operator: if set to CLOSED the whole workflow is executed; if OPEN, metrics are computed but aggregation/consolidation is not performed
  // challenge_status = "CLOSED"

  // directories where results will be written
  output = "${params.participant_id}_benchmark_results"
  validation_result = "${params.output}/validated_participant_data.json"
  assessment_results = "${params.output}/assessment_datasets.json"
  results = "${params.output}/results"
  statsdir = "${params.output}/stats"
  consolidation_result = "${params.output}/consolidated_results.json"
  otherdir = "${params.output}/other"

  // other parameters
  windows = "100 50 10" // type int, several values inside string separated by spaces. The first value is used for relative PAS usage calculation.
  genome_dir = "$goldstandard_dir/genomes" // Must contain a .gtf file with same substring as in challenge_ids, e.g. test.genome.mm10_test.gtf
  offline = 0 // type int; 0 for False

  //set DEFAULT_eventMark
  event_date = '2022-08-10'
}

// By default output execution reports
timeline {
  enabled = true
  file = "${params.statsdir}/timeline.html"
}
report {
  enabled = true
  file = "${params.statsdir}/report.html"
}
trace {
  enabled = true
  file = "${params.statsdir}/trace.txt"
}
dag {
  enabled = true
  file = "${params.statsdir}/DAG.dot"
}
